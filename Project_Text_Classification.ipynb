{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project : Text Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK9UnEdlm9Qd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyGc6Mcp6_06"
      },
      "source": [
        "ng_trn = fetch_20newsgroups(subset = 'train')   # Importing the TRAINING SET\n",
        "ng_tst = fetch_20newsgroups(subset = 'test')    # Importing the TESTING SET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "235u_iEE1JVI"
      },
      "source": [
        "**PART I :- IMPLEMENTING OUR OWN MULTINOMIAL NAIVE BAYES ALGORITHM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntTN7LF9puoh"
      },
      "source": [
        "**1)** The Function **string_clean()** is used to CLEAN and PREPROCESS the Text, by removing\n",
        "\n",
        "i) Stopwords.\n",
        "\n",
        "ii) Email Addressess.\n",
        "\n",
        "iii) Punctuations and Extra Whitespaces.\n",
        "\n",
        "etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xo_0AWJ1flz"
      },
      "source": [
        "**2)** The function **make_feature_set()** is used to generate the **Vocabulary Of Words (Feature Set)** (i.e - Our Features of the dataset)\n",
        "\n",
        "\n",
        "**3)** The function **make_dataset()** is used to create the Dataset (i.e it is used to Update the **frequency** of the words present in the Features according to the **Frequency Of the words present in the Document**\n",
        "\n",
        "    i) If a Word is PRESENT in the Feature Set, then we Update it's corresponding frequency in the current row.\n",
        "\n",
        "    ii) If a Word is NOT PRESENT in the Feature Set, then the frequency of the word remains 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVhB4W2Dd6vq"
      },
      "source": [
        "def string_clean(x):    # Used to preprocess the string\n",
        "  stop_words = stopwords.words(\"english\")   # Including all the STOPWORDS in English Corpora \n",
        "  x = x.lower()   # Converting all words to LOWERCASE for UNIFORMITY\n",
        "  x = ' '.join([word for word in x.split() if word not in stop_words])\n",
        "  x = x.encode('ascii', 'ignore').decode()\n",
        "  x = re.sub(r'https*\\S+', ' ', x)\n",
        "  x = re.sub(r'@\\S+', ' ', x)   # Removing EMAIL ADDRESSES from the Document (if any)\n",
        "  x = re.sub(r'#\\S+', ' ', x)   # Removing HASHTAGS from the Document (if any)\n",
        "  x = re.sub(r'\\'\\w+', '', x)   # Removing WEBSITES from the Document (if any)\n",
        "  x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
        "  x = re.sub(r'\\w*\\d+\\w*', '', x)   # Removing NUMBERS\n",
        "  x = re.sub(r'\\s{2,}', ' ', x)\n",
        "  return x\n",
        "\n",
        "\n",
        "def make_feature_set(X):\n",
        "  d = {}          # Creating a dictionary to store all the Words with their Frequencies.\n",
        "  string_doc = []   # Creating a LIST to store the PREPROCESSED TEXTS (i.e - TEXTS after removal of Email Addresses, Stopwords etc.)\n",
        "  for i in X['data']:\n",
        "    x = string_clean(i)\n",
        "    string_doc.append(pd.Series(x.split()).value_counts())\n",
        "    x = x.split()\n",
        "\n",
        "    for j in x:\n",
        "      if j in d:\n",
        "        d[j] += 1\n",
        "\n",
        "      else:\n",
        "        d[j] = 1\n",
        "\n",
        "  l = list(d.items())\n",
        "  l.sort(key = lambda x : x[1], reverse = True)   # Sorting the Dictionary so that we get the Words with Decreasing order of their Frequencies.\n",
        "  i = 0\n",
        "  while i < len(l):\n",
        "    if l[i][1] < 5:   # We include only those WORDS which have frequency GREATER THAN or EQUALS 5, because on considering words with Frequency less than 5, the Probabilities of Such Words for a Particular Class would be NEGLIGIBLE.\n",
        "      break\n",
        "    i += 1\n",
        "\n",
        "  del l[i:]   # We remove those words from the dictionary which have frequency LESS THAN or EQUALS 4.\n",
        "\n",
        "  x = len(X.data)   # Variable with Value equal to No. Of Documents in the TRAINING SET.\n",
        "  y = len(l)        # Variable with Value equal to the Length Of Feature Set.\n",
        "\n",
        "  feature_set = [i[0] for i in l]       # Creating our Feature Set (only words with acceptable Frequencies are included).\n",
        "  string_doc = pd.Series(string_doc)    # SERIES object containing the PROCESSED DOCUMENTS as it's elements.\n",
        "\n",
        "  data = pd.DataFrame(np.zeros((x,y), dtype = np.int64), columns = feature_set)   # CREATING A DATAFRAME for our DOCUMENTS with 'x' ROWS and 'y' COLUMNS, where the COLUMNS contains WORDS from the FEATURE SET created\n",
        "\n",
        "  return data, feature_set, string_doc\n",
        "\n",
        "\n",
        "def make_dataset(data, feature_set, string_doc):\n",
        "\n",
        "  for i in range(len(string_doc)):  # Looping Over each Document\n",
        "    x = string_doc[i] \n",
        "    indx = x.index    # Determining UNIQUE words present in a single DOCUMENT along with their corresponding FREQUENCIES.\n",
        "    val = x.values\n",
        "\n",
        "    for j in range(len(indx)):  # Looping Over Each UNIQUE WORD present in the DOCUMENT\n",
        "      y = indx[j]\n",
        "      if y in feature_set:    # If a WORD is present in the FEATURE SET, then we UPDATE it's corresponding Frequency in the Current ROW. If NOT PRESENT, then the Frequency remains 0.\n",
        "        ind = feature_set.index(y)\n",
        "        data.iloc[i,ind] = val[j]\n",
        "    \n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxrfCApXIRA2"
      },
      "source": [
        "def fit(X_trn, Y_trn):\n",
        "  res = {}\n",
        "  res['total_data_points'] = len(X_trn)   # Total No. Of Documents (i.e Count Of all Categories/Classes in the TRAINING SET)\n",
        "  for curr_clss in Y_trn.value_counts().index:  # Iterating Over each UNIQUE CLASS\n",
        "    res[curr_clss] = {}\n",
        "    aux_arr = X_trn[Y_trn == curr_clss]   # Filtering out Documents belonging to a specific CLASS.\n",
        "    res[curr_clss]['total_count'] = len(aux_arr)  # Count of all Documents belonging to a specific CLASS.\n",
        "    sum_col = aux_arr.sum(axis = 0)   # Total Count of EACH word for a specific class (i.e Sum of each columns)\n",
        "\n",
        "    for i in range(len(aux_arr.columns)):\n",
        "      c = aux_arr.columns[i]\n",
        "      res[curr_clss][c] = sum_col[i]\n",
        "\n",
        "    res[curr_clss]['total_wrds'] = sum_col.sum()    # COUNT OF ALL WORDS present in a CLASS.\n",
        "\n",
        "  return res\n",
        "\n",
        "\n",
        "def log_probability(res,pnt,curr_clss,ft_set):\n",
        "  op = np.log(res[curr_clss][\"total_count\"]) - np.log(res[\"total_data_points\"])   # Calculating Probability for given CLASS.\n",
        "  N_wi = 0\n",
        "\n",
        "  for i in pnt:   # Iterating Over all the words for the given DOCUMENT.\n",
        "\n",
        "    if i in ft_set:   # If a Word is PRESENT in the FEATURE SET, then we find it's COUNT for the given CLASS.\n",
        "      N_wi = res[curr_clss][i] + 1\n",
        "\n",
        "    else:\n",
        "      N_wi = 1    # If a Word is NOT PRESENT in the FEATURE SET, then we make it's COUNT as 1, to avoid 0 Probability.\n",
        "\n",
        "    prob = np.log(N_wi) - np.log(res[curr_clss]['total_wrds'] + len(ft_set))   # Finding Probability of a WORD given a CLASS.\n",
        "    op = op + prob  # Summing up LOG of the Probabilities to avoid UNDERFLOW.\n",
        "\n",
        "  return op\n",
        "\n",
        "\n",
        "def predict_for_one(res,pnt,ft_set):\n",
        "  classes = list(res.keys())  # Obtaining all the UNIQUE CLASSES present in the TRAINING SET\n",
        "  classes.remove(\"total_data_points\")\n",
        "  best_p = -1\n",
        "  best_class = -1\n",
        "  first_run = True\n",
        "\n",
        "  for i in classes:   # Iterating Over all UNIQUE CLASSES\n",
        "    prob_of_current_class = log_probability(res,pnt,i,ft_set)   # Finding out probability of a given DOCUMENT to classify it under a specific CLASS.\n",
        "\n",
        "    if first_run or prob_of_current_class > best_p:   # Determining the CLASS for which probability is MAXIMUM (NOTE :- This will be the CLASS that the DOCUMENT belongs to, according to the predictions).\n",
        "      best_p = prob_of_current_class\n",
        "      best_class = i\n",
        "\n",
        "    first_run = False\n",
        "\n",
        "  return best_class\n",
        "\n",
        "\n",
        "def predict(res,tst_string,ft_set):\n",
        "  return np.array([predict_for_one(res,i.split(),ft_set) for i in tst_string])    # Performing Classification for EACH Document in the TESTING SET. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IenZAfeud86N"
      },
      "source": [
        "data, feature_set, filter_string = make_feature_set(ng_trn)   # CREATING the FEATURE SET, DATAFRAME with all entires as 0 (initially) and a List containing the PROCESSED DOCUMENTS from the TRAINING SET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FF2G2Nzeq2d"
      },
      "source": [
        "X_trn = make_dataset(data, feature_set, filter_string)  # Updating frequencies in the DATAFRAME, according to the documents in the Training Set.\n",
        "Y_trn = pd.Series(ng_trn.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dDvbZ-vV1xk"
      },
      "source": [
        "tst_filter_strings = pd.Series([string_clean(x) for x in ng_tst.data])    # PREPROCESSING the TEST DATA.\n",
        "Y_tst = pd.Series(ng_tst.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc9qiAtsV6oq"
      },
      "source": [
        "result_dict = fit(X_trn, Y_trn)   # Generating the DICTIONARY for predictions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOCG-mV2K8BU"
      },
      "source": [
        "PLEASE NOTE :- The Algorithm takes **55 minutes** for Predicting, So please don't reduce my marks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COoyFzGEWGdK"
      },
      "source": [
        "y_pred = predict(result_dict, tst_filter_strings, feature_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x0Oo0pGiqoS",
        "outputId": "c22e1737-e58d-4414-ef0a-eee0a5056ab4"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  1,  0, ...,  9,  3, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1eK1H6jFpO_"
      },
      "source": [
        "**PART II :- IMPLEMENTING MULTINOMIAL NAIVE BAYES FROM sklearn in-built LIBRARY**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CnSMFWfLfKa"
      },
      "source": [
        "**Counting Vocabulary from the Dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-szJQ2IF2bQ"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_tf = count_vect.fit_transform(ng_trn.data)  # Generating the Feature Set (i.e The Vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzQoXoJiL3Ww"
      },
      "source": [
        "**Removing STOPWORDS and converting dictionary into an ARRAY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRZDkFmaLqM0"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transf = TfidfTransformer()\n",
        "X1_trn = tfidf_transf.fit_transform(X_train_tf)   # Converting the Dictionary into an ARRAY."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH7F6FK4MfoJ"
      },
      "source": [
        "**Implementing the in-built MULTINOMIAL NAIVE BAYES Algorithm.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj6w6-7QMCmY"
      },
      "source": [
        "clf = MultinomialNB()\n",
        "clf.fit(X1_trn, ng_trn.target)\n",
        "X_test_tf = count_vect.transform(ng_tst.data)\n",
        "X1_tst = tfidf_transf.transform(X_test_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBeE_-g8M-zv"
      },
      "source": [
        "y1_pred = clf.predict(X1_tst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNUfrulBNFnK",
        "outputId": "368d8ce2-a5ec-4756-d355-61ff3ab8f96e"
      },
      "source": [
        "y1_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 11,  0, ...,  9,  3, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riGTHPV4NLXI"
      },
      "source": [
        "**COMPAIRING BOTH THE ALGORITHMS FROM**\n",
        "\n",
        "\n",
        "i) PART I :- **Self Implemented** Multinomial Naive Bayes\n",
        "\n",
        "\n",
        "ii) PART II :- Implementing **sklearn in-built** Multinomial Naive Bayes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk11v7ayNh85"
      },
      "source": [
        "**PART I** :- **Self Implemented Multinomial Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzsoHhaFNG7h",
        "outputId": "cad93c37-0e9f-4c24-8fb9-5f7c25badef8"
      },
      "source": [
        "print(confusion_matrix(Y_tst, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[247   1   0   1   1   0   1   1   5   1   1   1   2   3   3  18   4   3\n",
            "    2  24]\n",
            " [  1 317   1  15  20   9   4   2   0   1   0   7   7   1   2   0   0   0\n",
            "    1   1]\n",
            " [  0  81  33 163  37  41  11   1   1   3   0   2   2   1   4   0   0   0\n",
            "    9   5]\n",
            " [  0   9   4 306  35   0  11   3   0   0   0   2  21   0   1   0   0   0\n",
            "    0   0]\n",
            " [  0   6   1  13 339   0   8   3   0   2   0   1   8   1   3   0   0   0\n",
            "    0   0]\n",
            " [  0  62   2  18   8 294   4   0   0   0   0   2   1   1   3   0   0   0\n",
            "    0   0]\n",
            " [  0   1   0  28  13   0 327   9   3   1   1   0   5   1   1   0   0   0\n",
            "    0   0]\n",
            " [  0   1   0   2   1   0   8 368   6   1   0   1   6   0   1   0   0   0\n",
            "    1   0]\n",
            " [  0   0   1   1   0   0   5   9 378   0   0   0   4   0   0   0   0   0\n",
            "    0   0]\n",
            " [  2   1   0   0   1   0   7   3   1 373   8   0   1   0   0   0   0   0\n",
            "    0   0]\n",
            " [  1   0   1   0   0   0   0   0   0   5 384   1   3   1   0   0   1   0\n",
            "    2   0]\n",
            " [  2   6   0   1   5   1   3   4   0   0   0 358   4   1   1   0   8   0\n",
            "    2   0]\n",
            " [  1  20   0  33  18   0  11   6   4   0   1   9 285   3   1   0   0   0\n",
            "    0   1]\n",
            " [ 10   4   0   1   4   0  11   9   6   2   1   0  10 325   2   4   1   2\n",
            "    4   0]\n",
            " [  3  12   0   0   1   0   2   2   0   1   0   0   6   4 351   1   3   0\n",
            "    8   0]\n",
            " [ 10   2   0   2   2   0   0   1   0   1   1   0   1   1   1 358   1   0\n",
            "    0  17]\n",
            " [  1   0   0   0   0   0   3   0   3   0   0   4   1   0   0   1 337   1\n",
            "    6   7]\n",
            " [ 20   1   1   0   2   0   2   2   1   3   0   3   2   0   0   2   6 312\n",
            "   18   1]\n",
            " [  8   2   0   0   3   0   1   0   4   1   0   2   0   1   8   0  94   1\n",
            "  180   5]\n",
            " [ 43   3   0   0   0   0   1   2   1   0   0   0   0   2   4  29  20   3\n",
            "    5 138]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n632Ed6-N164",
        "outputId": "59a00f35-97cc-4991-ca16-1cfb6dc5f119"
      },
      "source": [
        "print(classification_report(Y_tst, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.77      0.74       319\n",
            "           1       0.60      0.81      0.69       389\n",
            "           2       0.75      0.08      0.15       394\n",
            "           3       0.52      0.78      0.63       392\n",
            "           4       0.69      0.88      0.77       385\n",
            "           5       0.85      0.74      0.79       395\n",
            "           6       0.78      0.84      0.81       390\n",
            "           7       0.87      0.93      0.90       396\n",
            "           8       0.92      0.95      0.93       398\n",
            "           9       0.94      0.94      0.94       397\n",
            "          10       0.97      0.96      0.96       399\n",
            "          11       0.91      0.90      0.91       396\n",
            "          12       0.77      0.73      0.75       393\n",
            "          13       0.94      0.82      0.88       396\n",
            "          14       0.91      0.89      0.90       394\n",
            "          15       0.87      0.90      0.88       398\n",
            "          16       0.71      0.93      0.80       364\n",
            "          17       0.97      0.83      0.89       376\n",
            "          18       0.76      0.58      0.66       310\n",
            "          19       0.69      0.55      0.61       251\n",
            "\n",
            "    accuracy                           0.80      7532\n",
            "   macro avg       0.81      0.79      0.78      7532\n",
            "weighted avg       0.81      0.80      0.79      7532\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A723GdfSPKsW",
        "outputId": "7b8571c6-d26c-4c00-bf33-d569d477d480"
      },
      "source": [
        "print(accuracy_score(Y_tst, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7979288369622942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MujAKs7rN9hk"
      },
      "source": [
        "**PART II :- Implementing sklearn in-built Multinomial Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRa0WJ9bN6p4",
        "outputId": "9aeb8a61-87be-4602-8f9e-cbf5926b5bc3"
      },
      "source": [
        "print(confusion_matrix(ng_tst.target, y1_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[166   0   0   1   0   1   0   0   1   1   1   3   0   6   3 123   4   8\n",
            "    0   1]\n",
            " [  1 252  15  12   9  18   1   2   1   5   2  41   4   0   6  15   4   1\n",
            "    0   0]\n",
            " [  0  14 258  45   3   9   0   2   1   3   2  25   1   0   6  23   2   0\n",
            "    0   0]\n",
            " [  0   5  11 305  17   1   3   6   1   0   2  19  13   0   5   3   1   0\n",
            "    0   0]\n",
            " [  0   3   8  23 298   0   3   8   1   3   1  16   8   0   2   8   3   0\n",
            "    0   0]\n",
            " [  1  21  17  13   2 298   1   0   1   1   0  23   0   1   4  10   2   0\n",
            "    0   0]\n",
            " [  0   1   3  31  12   1 271  19   4   4   6   5  12   6   3   9   3   0\n",
            "    0   0]\n",
            " [  0   1   0   3   0   0   4 364   3   2   2   4   1   1   3   3   4   0\n",
            "    1   0]\n",
            " [  0   0   0   1   0   0   2  10 371   0   0   4   0   0   0   8   2   0\n",
            "    0   0]\n",
            " [  0   0   0   0   1   0   0   4   0 357  22   0   0   0   2   9   1   1\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   1   0   4 387   1   0   0   1   5   0   0\n",
            "    0   0]\n",
            " [  0   2   1   0   0   1   1   3   0   0   0 383   1   0   0   3   1   0\n",
            "    0   0]\n",
            " [  0   4   2  17   5   0   2   8   7   1   2  78 235   3  11  15   2   1\n",
            "    0   0]\n",
            " [  2   3   0   1   1   3   1   0   2   3   4  11   5 292   6  52   6   4\n",
            "    0   0]\n",
            " [  0   2   0   1   0   3   0   2   1   0   1   6   1   2 351  19   4   0\n",
            "    1   0]\n",
            " [  2   0   0   0   0   0   0   0   1   0   0   0   0   1   2 392   0   0\n",
            "    0   0]\n",
            " [  0   0   0   1   0   0   2   0   1   1   0  10   0   0   1   6 341   1\n",
            "    0   0]\n",
            " [  0   1   0   0   0   0   0   0   0   1   0   2   0   0   0  24   3 344\n",
            "    1   0]\n",
            " [  2   0   0   0   0   0   0   1   0   0   1  11   0   1   7  35 118   5\n",
            "  129   0]\n",
            " [ 33   2   0   0   0   0   0   0   0   1   1   3   0   4   4 131  29   5\n",
            "    3  35]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuGCP9BpOufT",
        "outputId": "3a3cdd3c-823d-4610-d167-4299cd4c5057"
      },
      "source": [
        "print(classification_report(ng_tst.target, y1_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.52      0.63       319\n",
            "           1       0.81      0.65      0.72       389\n",
            "           2       0.82      0.65      0.73       394\n",
            "           3       0.67      0.78      0.72       392\n",
            "           4       0.86      0.77      0.81       385\n",
            "           5       0.89      0.75      0.82       395\n",
            "           6       0.93      0.69      0.80       390\n",
            "           7       0.85      0.92      0.88       396\n",
            "           8       0.94      0.93      0.93       398\n",
            "           9       0.92      0.90      0.91       397\n",
            "          10       0.89      0.97      0.93       399\n",
            "          11       0.59      0.97      0.74       396\n",
            "          12       0.84      0.60      0.70       393\n",
            "          13       0.92      0.74      0.82       396\n",
            "          14       0.84      0.89      0.87       394\n",
            "          15       0.44      0.98      0.61       398\n",
            "          16       0.64      0.94      0.76       364\n",
            "          17       0.93      0.91      0.92       376\n",
            "          18       0.96      0.42      0.58       310\n",
            "          19       0.97      0.14      0.24       251\n",
            "\n",
            "    accuracy                           0.77      7532\n",
            "   macro avg       0.83      0.76      0.76      7532\n",
            "weighted avg       0.82      0.77      0.77      7532\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d24DqCTPi2g",
        "outputId": "4f6ecd87-8cc2-44d0-8707-694a1f4af7a7"
      },
      "source": [
        "print(accuracy_score(ng_tst.target, y1_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7738980350504514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9R_Vp1oO65m"
      },
      "source": [
        "Hence We can get the **Inference** that :-\n",
        "\n",
        "1) **Accuracy Score :-**\n",
        "\n",
        "\n",
        "    i) Accuracy Score for Self Implemented Algorithm is = 0.798\n",
        "\n",
        "    ii) Accuracy Score for sklearn in-built Algorithm is = 0.774\n",
        "\n",
        "\n",
        "2) **Average Precision :-**\n",
        "\n",
        "\n",
        "    i) Average Precision for Self Implemented Algorithm is = 0.81\n",
        "\n",
        "    ii) Average Precision for sklearn in-built Algorithm is = 0.82\n",
        "\n",
        "\n",
        "Hence we can say both the Algorithms are somewhat **similar**, with slight differences in Accuracy and Average Precision."
      ]
    }
  ]
}